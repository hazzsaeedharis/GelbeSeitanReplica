# PostgreSQL + Elasticsearch Setup Guide

## ðŸŽ¯ Overview

This guide explains how to migrate from the NDJSON file-based search to PostgreSQL + Elasticsearch for production-grade performance.

## ðŸ“Š Benefits

### Current (NDJSON File)
- âŒ Slow (reads entire 200MB+ file for each search)
- âŒ No fuzzy matching
- âŒ No geo-distance search
- âŒ Limited scalability

### With PostgreSQL + Elasticsearch
- âœ… **100x faster** search queries
- âœ… **Fuzzy matching** (typo tolerance)
- âœ… **Geo-distance search** (find businesses within X km)
- âœ… **Full-text search** with German stemming
- âœ… **Autocomplete** for cities
- âœ… **Scalable** to millions of records

---

## ðŸš€ Quick Start with Docker

### Step 1: Start Services
```bash
cd backend
docker-compose up -d
```

This starts:
- **PostgreSQL** (port 5432) with PostGIS
- **Elasticsearch** (port 9200)
- **Kibana** (port 5601) - optional web UI
- **Redis** (port 6379) - for caching

### Step 2: Verify Services
```bash
# Check PostgreSQL
docker exec gelbeseiten_postgres pg_isready

# Check Elasticsearch
curl http://localhost:9200/_cluster/health

# Check Redis
docker exec gelbeseiten_redis redis-cli ping
```

### Step 3: Install Python Dependencies
```bash
# Activate virtual environment
source venv/bin/activate

# Install full dependencies (includes PostgreSQL, Elasticsearch, Redis)
pip install -r requirements-full.txt
```

### Step 4: Run Database Migration
```bash
# Test with 100 records first
python scripts/migrate_data_to_db.py --limit 100

# If successful, migrate all data (takes ~30-60 minutes for 200MB)
python scripts/migrate_data_to_db.py
```

### Step 5: Update API to Use Database
```bash
# The API automatically uses PostgreSQL + Elasticsearch when available
# Just restart the server:
uvicorn app.main:app --reload --port 8000
```

---

## ðŸ“¦ What Gets Created

### PostgreSQL Tables
```sql
-- Main business table with PostGIS location
businesses (
    id, name, street, house_number, postcode, city,
    phone, email, website, location (POINT),
    created_at, updated_at
)

-- Branch/category table
branches (
    branch_id, name, description
)

-- Many-to-many relationship
business_branches (
    business_id, branch_id
)
```

### Elasticsearch Index
```json
{
  "businesses": {
    "mappings": {
      "name": "text with German analyzer",
      "city": "text with autocomplete",
      "location": "geo_point",
      "branches": "text array"
    }
  }
}
```

### Indexes Created
- **PostgreSQL**: name (trigram), city (trigram), postcode, location (GIST)
- **Elasticsearch**: full-text, geo-point, autocomplete
- **Redis**: Query result caching

---

## ðŸ” Enhanced Search Features

### 1. Fuzzy Matching
```bash
# Before (NDJSON): "arst" finds nothing
# After (Elasticsearch): "arst" finds "Arzt"

curl "http://localhost:8000/api/v2/search?keyword=arst&location=berlin"
```

### 2. Geo-Distance Search
```bash
# Find businesses within 5km of coordinates
curl "http://localhost:8000/api/v2/search?lat=52.52&lon=13.405&radius=5"
```

### 3. Autocomplete
```bash
# City autocomplete
curl "http://localhost:8000/api/v2/autocomplete/cities?prefix=ber"
# Returns: ["Berlin", "Bergisch Gladbach", "Bernau", ...]
```

### 4. Advanced Sorting
```bash
# Sort by distance
curl "http://localhost:8000/api/v2/search?keyword=arzt&lat=52.52&lon=13.405&sort_by=distance"

# Sort by name
curl "http://localhost:8000/api/v2/search?keyword=arzt&location=berlin&sort_by=name"
```

---

## ðŸ“ˆ Performance Comparison

| Operation | NDJSON File | PostgreSQL + ES |
|-----------|-------------|-----------------|
| Search "arzt berlin" | ~2-5 seconds | ~50-100ms |
| Geo-distance search | Not supported | ~100ms |
| Autocomplete | Not supported | ~20ms |
| Fuzzy search | Not supported | ~80ms |
| Pagination | Fast (in-memory) | Fast (indexed) |

---

## ðŸ› ï¸ Migration Script Options

### Test Migration (100 records)
```bash
python scripts/migrate_data_to_db.py --limit 100
```

### Full Migration
```bash
# This will take 30-60 minutes for 200MB data
python scripts/migrate_data_to_db.py
```

### Migration Process:
1. Reads NDJSON file line by line
2. Geocodes addresses (with caching)
3. Inserts into PostgreSQL
4. Indexes in Elasticsearch
5. Shows progress every 100 records

---

## ðŸ“Š Database Stats

After migration, check stats:
```bash
curl http://localhost:8000/api/v2/stats
```

Response:
```json
{
  "total_businesses": 50000,
  "total_branches": 1500,
  "businesses_with_location": 48000,
  "businesses_with_phone": 35000,
  "unique_cities": 2000,
  "geocoding_coverage": 96.0
}
```

---

## ðŸ”§ Configuration

### Environment Variables (.env)
```env
# PostgreSQL
DATABASE_URL=postgresql://gs_user:gs_password@localhost:5432/gelbeseiten

# Elasticsearch
ELASTICSEARCH_HOST=localhost
ELASTICSEARCH_PORT=9200

# Redis
REDIS_URL=redis://localhost:6379

# API Settings
USE_ELASTICSEARCH=true
USE_REDIS_CACHE=true
CACHE_TTL=300  # 5 minutes
```

---

## ðŸ§ª Testing the New Search

### 1. Start Docker Services
```bash
docker-compose up -d
```

### 2. Migrate Sample Data
```bash
python scripts/migrate_data_to_db.py --limit 1000
```

### 3. Test Elasticsearch
```bash
# Check index
curl http://localhost:9200/businesses/_count

# Search directly in ES
curl -X POST http://localhost:9200/businesses/_search \
  -H 'Content-Type: application/json' \
  -d '{
    "query": {
      "multi_match": {
        "query": "arzt",
        "fields": ["name^2", "branches"],
        "fuzziness": "AUTO"
      }
    }
  }'
```

### 4. Test PostgreSQL
```bash
# Connect to database
docker exec -it gelbeseiten_postgres psql -U gs_user -d gelbeseiten

# Run queries
SELECT COUNT(*) FROM businesses;
SELECT COUNT(*) FROM branches;
SELECT * FROM business_search_view LIMIT 5;
```

---

## ðŸš€ Deployment Checklist

- [ ] Start Docker services
- [ ] Run database migration
- [ ] Verify data in PostgreSQL
- [ ] Verify data in Elasticsearch
- [ ] Update API to use new endpoints
- [ ] Test search performance
- [ ] Update frontend to use v2 API
- [ ] Monitor query performance
- [ ] Set up backups

---

## ðŸ“ Maintenance

### Backup Database
```bash
docker exec gelbeseiten_postgres pg_dump -U gs_user gelbeseiten > backup.sql
```

### Restore Database
```bash
docker exec -i gelbeseiten_postgres psql -U gs_user gelbeseiten < backup.sql
```

### Reindex Elasticsearch
```bash
# Delete and recreate index
curl -X DELETE http://localhost:9200/businesses
python -c "from app.elasticsearch_client import init_elasticsearch; init_elasticsearch()"

# Re-run migration
python scripts/migrate_data_to_db.py
```

---

## âš¡ Performance Tuning

### PostgreSQL
```sql
-- Analyze tables
ANALYZE businesses;
ANALYZE branches;

-- Vacuum
VACUUM ANALYZE businesses;
```

### Elasticsearch
```bash
# Refresh index
curl -X POST http://localhost:9200/businesses/_refresh

# Force merge segments
curl -X POST http://localhost:9200/businesses/_forcemerge?max_num_segments=1
```

---

## ðŸ“š Resources

- **PostgreSQL Docs**: https://www.postgresql.org/docs/
- **PostGIS**: https://postgis.net/documentation/
- **Elasticsearch**: https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html
- **SQLAlchemy**: https://docs.sqlalchemy.org/
- **GeoAlchemy2**: https://geoalchemy-2.readthedocs.io/

---

**Ready to migrate?** Just run: `docker-compose up -d && python scripts/migrate_data_to_db.py --limit 100`
